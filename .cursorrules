# Opportunity Scraper System Project Rules

## Project Context
This is a TypeScript/Node.js application that automates professional opportunity discovery and workflow management using local AI (Ollama + Llama 3.1 8B). The system manages two parallel workflows: **Jobs** (application automation) and **Leads** (relationship building).

### Jobs Workflow Features
- **AI-Powered Job Ranking**: Weighted multi-category scoring (Azure/API 20%, Security 15%, .NET 20%, Event-Driven 10%, Performance 10%, Frontend 10%, Legacy Modernization 5%)
- **Rejection Learning System**: Automatically adjusts weights and filters based on rejection reasons using keyword patterns and LLM analysis
- **Intelligent Form Automation**: Three-tier mapping system (heuristics → cache → AI) with ATS detection for Greenhouse, Lever, Workday
- **Selector Learning System**: Captures and learns CSS selectors from successful form fills
- **Job Status Lifecycle**: Six-state tracking (queued, applied, interview, rejected, skipped, reported)

### Leads Workflow Features
- **LinkedIn Lead Scraping**: Automated People Search scraping with connection degree filtering (1st, 2nd, 3rd)
- **Profile-Based Discovery**: Pre-defined search strategies targeting specific professional segments (chiefs, core, security, etc.)
- **Contact Enrichment**: Extracts emails, phones, birthdays, work history, and social profiles
- **AI Background Generation**: Creates professional context summaries for personalized outreach
- **Email Template Generation**: AI-powered introduction and follow-up templates based on lead background
- **Relationship Tracking**: Status management (not_contacted, email_sent, replied, meeting_scheduled)
- **Birthday Reminders**: Tracks upcoming birthdays for engagement opportunities

### Shared Infrastructure
- **Real-Time Dashboard**: React + Express with TanStack Query, HTTPS, auto-refresh monitoring for both jobs and leads
- **Profile-Based Search**: Reusable search strategies across both workflows
- **AI Content Generation**: Headlines, cover letters, backgrounds, and email templates
- **MCP Servers**: Playwright, Context7, and Azure MCP integrations for debugging and deployment

### Tech Stack
- TypeScript 5.6
- Playwright (browser automation)
- SQLite (better-sqlite3)
- Ollama + Llama 3.1 8B (local LLM)
- React 18 + Vite (dashboard frontend)
- Express (backend API with HTTPS on port 3001)
- TanStack Query (data fetching and caching)
- Tailwind CSS (styling)
- Zod (validation)

## AI-First Development Principles

This project is developed with an "AI-first" approach where code is optimized for AI comprehension, maintenance, and extension. As the codebase grows, AI assistants should be able to understand context, make changes confidently, and maintain consistency.

### Explicit Over Implicit
- Use descriptive, full-word names for variables, functions, and types (avoid abbreviations)
- Prefer `userProfileData` over `upd` or `data`
- Prefer `calculateJobFitScore` over `calcScore` or `getScore`
- Make relationships explicit: `getJobsByStatus('queued')` not `getJobs('q')`
- Avoid clever tricks or terse code - clarity beats brevity

### Strong, Explicit Typing
- Always export types and interfaces alongside implementations
- Define types for complex objects rather than inline type annotations
- Use discriminated unions for state machines (job status, application state)
- Annotate function parameters and return types explicitly, even when inferable
- Create type aliases for commonly used complex types

### Self-Documenting Structure
- Break complex functions into smaller, named helper functions
- Each function should do one thing with a clear name
- Use early returns to reduce nesting and clarify flow
- Group related functionality in clearly named files/modules
- File names should describe contents: `rejection-analyzer.ts` not `analyzer.ts`

### Context-Rich Comments
- Explain "why" decisions were made, not "what" the code does
- Document assumptions and constraints: "LinkedIn rate limits to 100 requests/hour"
- Note gotchas and edge cases: "Workday uses iframes, requires special handling"
- Reference related code or issues: "See lead-scraper.ts for similar pagination logic"
- Add JSDoc for exported functions with examples when helpful

### Predictable Patterns
- Establish patterns and follow them consistently throughout the codebase
- If adapters extend BaseAdapter, all adapters should follow this pattern
- If one route uses Zod validation, all routes should
- If one service uses try-catch-finally, all should
- Document patterns in this file so AI can follow them

### Explicit Error Handling
- Handle errors at each layer with context-specific messages
- Use custom error classes that carry context (jobId, url, step)
- Log errors with enough information for debugging
- Avoid generic catch-all error handlers
- Make error paths as explicit as success paths

### Clear State Management
- Make state transitions explicit and documented
- Use enums or const objects for state values, not magic strings
- Document valid state transitions: "queued can become applied, skipped, or reported"
- Validate state before transitions
- Store state history when debugging is needed

### Modular Architecture
- Extract reusable logic into clearly named utilities
- Keep business logic separate from UI/presentation
- Database queries in data layer, business logic in services, UI in components
- Avoid circular dependencies
- Each module should have a single, clear responsibility

### Examples and Patterns Over Documentation
- Include example usage in JSDoc comments
- Provide sample data structures in type definitions
- Show common patterns in comments: "// Example: { jobId: '123', status: 'queued' }"
- Keep a patterns file or section showing common approaches
- When adding new features, model them after existing similar features

### Database Schema Documentation
- Comment table purposes and relationships in schema
- Document column constraints and their business reasons
- Note migration considerations when schema changes
- Include sample queries for common operations
- Make foreign key relationships explicit in code

### Testable Design
- Write code that's easy to mock and test
- Avoid global state that makes testing hard
- Make dependencies injectable or mockable
- Keep side effects isolated and explicit
- Each function should be testable in isolation

### Progressive Disclosure
- Start with high-level overview comments in complex files
- Use section headers in long files to break into logical chunks
- Layer detail: summary at top, specifics below
- Help AI understand the forest before the trees
- Make it easy to find relevant code quickly

### Consistent Formatting
- Follow established code formatting (TypeScript/React conventions)
- Use consistent indentation and spacing
- Group imports logically (external, internal, types)
- Keep similar code structures aligned for easy scanning
- Use formatting to emphasize structure

## Code Style & Patterns

### TypeScript
- Use strict TypeScript with explicit types
- Prefer interfaces over types for object shapes
- Use type guards for runtime type checking
- Avoid `any` - use `unknown` if type is truly unknown
- Use async/await over raw promises
- Export types alongside implementations

### Error Handling
- Use custom error classes for domain-specific errors
- Always handle Playwright errors with appropriate retry logic
- Log errors with context (job ID, URL, step) for debugging
- Don't swallow errors silently - at minimum log them

### Database Operations
- Always use parameterized queries to prevent SQL injection
- Wrap multi-step operations in transactions
- Handle SQLITE_BUSY errors with retry logic
- Close database connections in finally blocks

### AI/LLM Integration
- Keep prompts in separate functions for maintainability
- Include error handling for API rate limits
- Cache expensive LLM calls when possible
- Structure prompts with clear instructions and examples
- Validate and sanitize LLM outputs before using

### Playwright Automation
- Use data-testid selectors when available
- Fall back to accessible selectors (role, label, text)
- Avoid brittle CSS selectors that may break with UI changes
- Take screenshots on failures for debugging
- Use waitForSelector with reasonable timeouts
- Store page state for debugging in artifacts/

### React/Dashboard
- Use TypeScript with React hooks
- Prefer composition over prop drilling (use contexts for shared state)
- Keep components focused and single-purpose
- Use Tailwind for styling (avoid custom CSS unless necessary)
- Handle loading and error states explicitly

## Testing Requirements
- Write tests for new features in both workflows (job ranking logic, lead scraping, AI generation)
- Mock external dependencies (Playwright, LLM APIs, LinkedIn pages)
- Test edge cases (missing data, API failures, timeout scenarios, CAPTCHA detection)
- Use descriptive test names that explain the scenario
- Keep tests isolated and repeatable
- Test both job and lead workflows when modifying shared components (AI, database, dashboard)

## Documentation
- Update docs/ when making architectural changes to either workflow
- Document new job adapters and lead scraping strategies
- Keep README.md current with setup instructions for both workflows
- Add JSDoc comments for exported functions with examples
- Explain "why" in comments, not "what" (code shows what)
- Document CLI commands and their options (jobs:search, leads:search)

## CLI Commands
- **Jobs**: `npm run jobs:search -- --profile <profile> [--max <number>]`
  - Profiles: core, security, event-driven, performance, devops, backend, core-net, legacy-modernization
  - Searches LinkedIn jobs, ranks, and queues high-scoring opportunities
- **Leads**: `npm run leads:search -- --profile <profile> [--max <number>] [--degree <1st|2nd|3rd>]`
  - Profiles: chiefs, core, security
  - Scrapes LinkedIn People Search with connection degree filtering
  - Default max: 1000 profiles (increased from 50)
- **Dashboard**: `npm run dev` (starts both frontend and backend with hot reload)
- **Scripts**: Various utility scripts in `scripts/` for database management, testing, and debugging

## File Organization
- Job-specific adapters: `src/adapters/` (ATS systems like Greenhouse, Lever, Workday)
- Lead scraping service: `src/services/lead-scraper.ts`
- AI/LLM logic: `src/ai/` (ranker, mapper, background-generator, email-templates, rejection-analyzer)
- Profile definitions: `src/ai/profiles.ts` (jobs), `src/ai/lead-profiles.ts` (leads)
- CLI commands: `src/cli/` (job-search.ts, lead-search.ts)
- Dashboard UI components: `src/dashboard/client/components/` (organize by feature: JobsList, LeadsList, etc.)
- Backend API routes: `src/dashboard/routes/` (jobs.ts, leads.ts, headline.ts, etc.)
- Database operations: `src/lib/db.ts` (all database queries and schemas)
- Keep test files adjacent to what they test when possible

## Security & Privacy
- Never commit credentials or API keys
- Use environment variables for sensitive config
- Sanitize user inputs before database storage
- Don't log sensitive information (passwords, tokens, LinkedIn session data)
- Handle PII carefully in logs:
  - Job data: applicant names, contact info in applications
  - Lead data: names, emails, phones, birthdays, work history
  - Minimize PII in error logs and debugging output
  - Respect soft deletion (deleted_at) throughout the system

## Performance
- Batch database operations when processing many jobs or leads
- Implement rate limiting for external APIs and LinkedIn scraping
- Use connection pooling for database (shared by both workflows)
- Cache static data (profiles, configuration, AI-generated content)
- Paginate large lists (jobs, leads) - avoid loading all records at once
- Cache AI-generated content (backgrounds, email templates) to avoid regeneration costs
- Use random delays in lead scraping to avoid LinkedIn rate limits and detection

## Common Pitfalls to Avoid
- Don't assume selectors will stay stable - LinkedIn and job sites change often
- Don't skip error handling in automation flows (jobs and leads)
- Don't forget to update both frontend and backend when changing data models
- Don't ignore TypeScript errors - fix them properly
- Don't add dependencies without considering bundle size
- Don't mix job and lead logic in shared components - keep workflows separate but use common utilities
- Don't forget to test LinkedIn scraping changes with real LinkedIn (not just mocks)
- Don't assume lead data is complete - handle missing fields gracefully (email, phone, birthday)

## When Making Changes
- Check if similar functionality already exists in either workflow (jobs or leads)
- Consider backward compatibility with existing database data (jobs, applications, leads)
- Update related tests for affected workflows
- Check if documentation needs updating (README.md, docs/)
- Test manually in dashboard if UI is affected (verify both jobs and leads sections if shared component)
- If modifying LinkedIn automation: test with real LinkedIn to catch selector changes
- If modifying AI prompts: verify output quality with sample data from both workflows

## System-Specific Features

### Jobs Workflow

#### Job Status Lifecycle
Six-state system: queued → applied → interview/rejected, plus skipped and reported
- **queued**: High-scoring jobs ready for application
- **applied**: Successfully submitted applications
- **interview**: Moved to interview stage (manual update)
- **rejected**: Application rejected (triggers learning system)
- **skipped**: Low score or failed application
- **reported**: Moved from queued for manual review, blocks auto-application

#### Rejection Learning System
Located in `src/ai/rejection-analyzer.ts`, `rejection-filters.ts`, `weight-manager.ts`
- Analyzes rejection reasons using keyword patterns and LLM
- Adjusts profile category weights immediately (e.g., "too junior" → increase seniority weight)
- Builds company blocklists for repeated rejections
- Filters jobs before ranking using learned patterns
- Dashboard displays active adjustments and learning history
- When modifying: ensure backward compatibility with existing learning data

#### Job Ranking & Scoring
Located in `src/ai/ranker.ts` and `profiles.ts`
- Current weights: Core Azure/API 20%, Security 15%, .NET 20%, Event-Driven 10%, Performance 10%, Frontend 10%, Legacy Modernization 5%, DevOps 0%
- MIN_FIT_SCORE default: 70 (configurable in .env)
- Profiles contain Boolean search queries and keyword lists per category
- Available profiles: core, security, event-driven, performance, devops, backend, core-net, legacy-modernization
- When changing weights: consider impact on existing queued jobs, may need re-scoring

#### AI Content Generation for Jobs
- **Headlines** (`src/dashboard/routes/headline.ts`): One-sentence professional summaries for applications
- **Cover Letters** (`src/dashboard/routes/cover-letter.ts`): Job-specific letters based on fit analysis
- Both use RAG context from resumes in `resumes/` folder
- Generated on-demand via dashboard, not pre-generated during search
- Use structured prompts with clear instructions and examples

#### Intelligent Field Mapping
Located in `src/ai/mapper.ts`
- Three-tier system: heuristics (fast) → cache (medium) → AI (accurate)
- Heuristics: Pattern matching for common fields (email, phone, name)
- Cache: Stores successful mappings in database
- AI: Semantic understanding for complex or ambiguous fields
- When modifying: test with all three tiers, ensure cache invalidation works

#### Selector Learning System
Located in `tests/learning-system/`
- Captures successful CSS selectors from form fills
- Stores in database for reuse
- Improves reliability over time as more forms are filled
- Run tests with: `npm run test:learning`
- When adding new adapters: ensure they integrate with learning system

#### ATS Adapters
Located in `src/adapters/`
- Must extend BaseAdapter interface
- Implement: detectJobBoard(), fillApplication()
- Each adapter handles board-specific quirks
- **Greenhouse**: Uses data-qa attributes, multi-step forms
- **Lever**: Single-page, autosaves, custom fields possible
- **Workday**: Uses iframes, slow loading, complex navigation
- Generic fallback for unknown boards

### Leads Workflow

#### Lead Scraping System
Located in `src/services/lead-scraper.ts`
- Scrapes LinkedIn People Search with filters (connection degree, location, title)
- Connection degree filtering: 1st (direct), 2nd (connections of connections), 3rd+ (extended network)
- Default max profiles: 1000 (configurable with --max flag)
- Rate limiting and resilience: random delays, CAPTCHA detection, session management
- Resume capability: can restart interrupted scraping runs
- When modifying: test with real LinkedIn to catch selector changes

#### Lead Profiles and Search Strategies
Located in `src/ai/lead-profiles.ts` and `src/cli/lead-search.ts`
- Profile-based searches targeting specific professional segments
- **chiefs**: C-level executives (CTO, CIO, CISO, VP Engineering)
- **core**: Technical leaders and architects in target technology stack
- **security**: Security-focused professionals (AppSec, CloudSec, DevSecOps)
- Each profile defines title keywords for filtering during scraping
- Profiles reuse job search keyword categories for consistency
- When adding profiles: follow established naming pattern, define clear target personas

#### Lead Data Model
Located in `src/lib/db.ts` (Lead interface)
- Core fields: name, title, company, about, location, profile_url
- Contact info: email, phone, website (social media handles)
- Relationship data: worked_together (previous collaboration indicator)
- Engagement hooks: birthday (format: "January 1"), connected_date
- Tracking: profile (search strategy used), email_status, scraped_at
- AI-generated: background (professional context for outreach)
- Soft deletion: deleted_at field for recoverable removal
- When extending schema: ensure backward compatibility with existing leads

#### AI Background Generation
Located in `src/ai/background-generator.ts`
- Generates professional context summaries from lead profile data
- Uses structured prompts focusing on title, company, about, and work history
- Two-paragraph format: professional role context + relationship building angle
- Cached in database to avoid regeneration costs
- Used as RAG context for email template generation
- When modifying: test with leads having varying amounts of profile data

#### Email Template System
Located in `src/ai/email-templates.ts`
- Generates personalized introduction and follow-up emails
- Template types: introduction, follow-up, birthday greeting
- Uses lead background, profile data, and relationship context
- Structured prompts with clear tone and length guidelines
- Generated on-demand via dashboard, not pre-generated
- When adding templates: follow established structure, include examples in prompts

#### Lead Status and Relationship Tracking
- **email_status** field: not_contacted (default), email_sent, replied, meeting_scheduled
- Manual status updates through dashboard UI
- Birthday reminders: queries upcoming birthdays (next 7 days) for engagement opportunities
- Soft deletion: preserves lead data while removing from active lists
- When adding status values: update Lead interface and all status-dependent queries

#### Lead Scraping Runs
Located in `src/lib/db.ts` (LeadScrapingRun interface)
- Tracks each scraping session: started_at, completed_at, status
- Progress metrics: profiles_scraped, profiles_added
- Configuration: filter_titles (JSON array), max_profiles
- Resume support: last_profile_url enables continuation after interruption
- Status values: in_progress, completed, stopped
- Dashboard displays run history and allows resuming stopped runs
- When modifying: ensure run resumption logic remains accurate

### Dashboard Features
Frontend: `src/dashboard/client/`, Backend: `src/dashboard/`
- Auto-refreshes every 5 seconds using TanStack Query
- HTTPS on localhost (ports 3000/3001)
- **Jobs Routes**: `/stats`, `/jobs`, `/runs`, `/analytics`, `/headline`, `/cover-letter`
- **Leads Routes**: `/leads`, `/leads/:id`, email template generation, birthday reminders
- Lead scraping modal: start new scraping runs with profile and filter selection
- Uses React contexts for navigation state
- Custom hooks in `hooks/` for data fetching (useJobs, useLeads, useUpcomingBirthdays)
- When adding features: update both frontend and backend, ensure HTTPS works

### MCP Servers Integration
- **Playwright MCP**: Interactive browser debugging (snapshots, clicks, network monitoring)
- **Context7 MCP**: Live documentation for Playwright, TypeScript, React
- **Azure MCP**: Cloud deployment and production monitoring
- Available for debugging complex automation issues
- Use browser_snapshot instead of screenshots for interactive debugging

### Testing Structure
- `tests/`: Main test suites (login, search, mapper, ranker, integration)
- `tests/learning-system/`: Selector learning tests
- Run all: `npm run test:all`
- Run learning only: `npm run test:learning`
- Tests use in-memory SQLite and mock LLM responses
- When adding tests: follow existing patterns, keep isolated

### Data Storage
- `data/app.db`: SQLite database (jobs, applications, leads, scraping runs, learning data)
  - Jobs tables: jobs, applications, scraping_runs, rejection_learning, field_mappings
  - Leads tables: leads, lead_scraping_runs
- `resumes/`: PDF/DOCX files for RAG context (used for both job applications and lead outreach)
- `storage/storageState.json`: LinkedIn session (gitignored, shared by job and lead scrapers)
- `artifacts/`: Screenshots and Playwright traces for debugging
- `dist/`: Built dashboard assets
- When modifying schema: handle migration carefully (SQLite limitations), test both workflows

### Configuration Files
- `.env`: Environment variables (MIN_FIT_SCORE, LLM_MODEL, HEADLESS, ENABLE_TRACING)
- `answers-policy.yml`: Controls application form responses (field length, allowed values)
- `docker-compose.llm.yml`: Ollama local LLM setup
- When changing config: update docs and provide migration path

