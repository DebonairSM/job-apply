# Opportunity Scraper System Project Rules

## Project Context
This is a TypeScript/Node.js application that automates professional opportunity discovery and workflow management using local AI (Ollama + Llama 3.1 8B). The system manages two parallel workflows: **Jobs** (application automation) and **Leads** (relationship building).

### Jobs Workflow Features
- **AI-Powered Job Ranking**: Weighted multi-category scoring (Azure/API 20%, Security 15%, .NET 20%, Event-Driven 10%, Performance 10%, Frontend 10%, Legacy Modernization 5%)
- **Rejection Learning System**: Automatically adjusts weights and filters based on rejection reasons using keyword patterns and LLM analysis
- **Intelligent Form Automation**: Three-tier mapping system (heuristics → cache → AI) with ATS detection for Greenhouse, Lever, Workday
- **Selector Learning System**: Captures and learns CSS selectors from successful form fills
- **Job Status Lifecycle**: Six-state tracking (queued, applied, interview, rejected, skipped, reported)

### Leads Workflow Features
- **LinkedIn Lead Scraping**: Automated People Search scraping with connection degree filtering (1st, 2nd, 3rd)
- **Profile-Based Discovery**: Pre-defined search strategies targeting specific professional segments (chiefs, core, security, etc.)
- **Contact Enrichment**: Extracts emails, phones, birthdays, work history, and social profiles
- **AI Background Generation**: Creates professional context summaries for personalized outreach
- **Email Template Generation**: AI-powered introduction and follow-up templates based on lead background
- **Relationship Tracking**: Status management (not_contacted, email_sent, replied, meeting_scheduled)
- **Birthday Reminders**: Tracks upcoming birthdays for engagement opportunities

### Shared Infrastructure
- **Real-Time Dashboard**: React + Express with TanStack Query, HTTPS, auto-refresh monitoring for both jobs and leads
- **Profile-Based Search**: Reusable search strategies across both workflows
- **AI Content Generation**: Headlines, cover letters, backgrounds, and email templates
- **MCP Servers**: Playwright, Context7, and Azure MCP integrations for debugging and deployment

### Tech Stack
- TypeScript 5.6
- Playwright (browser automation)
- SQLite (better-sqlite3)
- Ollama + Llama 3.1 8B (local LLM)
- React 18 + Vite (dashboard frontend)
- Express (backend API with HTTPS on port 3001)
- TanStack Query (data fetching and caching)
- Tailwind CSS (styling)
- Zod (validation)

## AI-First Development Principles

This project is developed with an "AI-first" approach where code is optimized for AI comprehension, maintenance, and extension. As the codebase grows, AI assistants should be able to understand context, make changes confidently, and maintain consistency.

### Explicit Over Implicit
- Use descriptive, full-word names for variables, functions, and types (avoid abbreviations)
- Prefer `userProfileData` over `upd` or `data`
- Prefer `calculateJobFitScore` over `calcScore` or `getScore`
- Make relationships explicit: `getJobsByStatus('queued')` not `getJobs('q')`
- Avoid clever tricks or terse code - clarity beats brevity

### Strong, Explicit Typing
- Always export types and interfaces alongside implementations
- Define types for complex objects rather than inline type annotations
- Use discriminated unions for state machines (job status, application state)
- Annotate function parameters and return types explicitly, even when inferable
- Create type aliases for commonly used complex types

### Self-Documenting Structure
- Break complex functions into smaller, named helper functions
- Each function should do one thing with a clear name
- Use early returns to reduce nesting and clarify flow
- Group related functionality in clearly named files/modules
- File names should describe contents: `rejection-analyzer.ts` not `analyzer.ts`

### Context-Rich Comments
- Explain "why" decisions were made, not "what" the code does
- Document assumptions and constraints: "LinkedIn rate limits to 100 requests/hour"
- Note gotchas and edge cases: "Workday uses iframes, requires special handling"
- Reference related code or issues: "See lead-scraper.ts for similar pagination logic"
- Add JSDoc for exported functions with examples when helpful

### Predictable Patterns
- Establish patterns and follow them consistently throughout the codebase
- If adapters extend BaseAdapter, all adapters should follow this pattern
- If one route uses Zod validation, all routes should
- If one service uses try-catch-finally, all should
- Document patterns in this file so AI can follow them

### Explicit Error Handling
- Handle errors at each layer with context-specific messages
- Use custom error classes that carry context (jobId, url, step)
- Log errors with enough information for debugging
- Avoid generic catch-all error handlers
- Make error paths as explicit as success paths

### Clear State Management
- Make state transitions explicit and documented
- Use enums or const objects for state values, not magic strings
- Document valid state transitions: "queued can become applied, skipped, or reported"
- Validate state before transitions
- Store state history when debugging is needed

### Modular Architecture
- Extract reusable logic into clearly named utilities
- Keep business logic separate from UI/presentation
- Database queries in data layer, business logic in services, UI in components
- Avoid circular dependencies
- Each module should have a single, clear responsibility

### Examples and Patterns Over Documentation
- Include example usage in JSDoc comments
- Provide sample data structures in type definitions
- Show common patterns in comments: "// Example: { jobId: '123', status: 'queued' }"
- Keep a patterns file or section showing common approaches
- When adding new features, model them after existing similar features

### Database Schema Documentation
- Comment table purposes and relationships in schema
- Document column constraints and their business reasons
- Note migration considerations when schema changes
- Include sample queries for common operations
- Make foreign key relationships explicit in code

### Testable Design
- Write code that's easy to mock and test
- Avoid global state that makes testing hard
- Make dependencies injectable or mockable
- Keep side effects isolated and explicit
- Each function should be testable in isolation

### Progressive Disclosure
- Start with high-level overview comments in complex files
- Use section headers in long files to break into logical chunks
- Layer detail: summary at top, specifics below
- Help AI understand the forest before the trees
- Make it easy to find relevant code quickly

### Consistent Formatting
- Follow established code formatting (TypeScript/React conventions)
- Use consistent indentation and spacing
- Group imports logically (external, internal, types)
- Keep similar code structures aligned for easy scanning
- Use formatting to emphasize structure

## Code Style & Patterns

### TypeScript
- Use strict TypeScript with explicit types
- Prefer interfaces over types for object shapes
- Use type guards for runtime type checking
- Avoid `any` - use `unknown` if type is truly unknown
- Use async/await over raw promises
- Export types alongside implementations

### Error Handling
- Use custom error classes for domain-specific errors
- Always handle Playwright errors with appropriate retry logic
- Log errors with context (job ID, URL, step) for debugging
- Don't swallow errors silently - at minimum log them

### Database Operations
- Always use parameterized queries to prevent SQL injection
- Wrap multi-step operations in transactions
- Handle SQLITE_BUSY errors with retry logic
- Close database connections in finally blocks

### AI/LLM Integration
- Keep prompts in separate functions for maintainability
- Include error handling for API rate limits
- Cache expensive LLM calls when possible
- Structure prompts with clear instructions and examples
- Validate and sanitize LLM outputs before using

### Playwright Automation
- Use data-testid selectors when available
- Fall back to accessible selectors (role, label, text)
- Avoid brittle CSS selectors that may break with UI changes
- Take screenshots on failures for debugging
- Use waitForSelector with reasonable timeouts
- Store page state for debugging in artifacts/

### React/Dashboard
- Use TypeScript with React hooks
- Prefer composition over prop drilling (use contexts for shared state)
- Keep components focused and single-purpose
- Use Tailwind for styling (avoid custom CSS unless necessary)
- Handle loading and error states explicitly

## Testing Requirements
- Write tests for new features, especially AI/ranking logic
- Mock external dependencies (Playwright, LLM APIs)
- Test edge cases (missing data, API failures, timeout scenarios)
- Use descriptive test names that explain the scenario
- Keep tests isolated and repeatable

## Documentation
- Update docs/ when making architectural changes
- Document new adapters and their specific requirements
- Keep README.md current with setup instructions
- Add JSDoc comments for exported functions
- Explain "why" in comments, not "what" (code shows what)

## File Organization
- Place new adapters in src/adapters/
- AI/LLM logic goes in src/ai/
- Dashboard UI components in src/dashboard/client/components/
- Backend API routes in src/dashboard/routes/
- Keep test files adjacent to what they test when possible

## Security & Privacy
- Never commit credentials or API keys
- Use environment variables for sensitive config
- Sanitize user inputs before database storage
- Don't log sensitive information (passwords, tokens)
- Handle PII (names, emails) carefully in logs

## Performance
- Batch database operations when processing many jobs
- Implement rate limiting for external APIs
- Use connection pooling for database
- Cache static data (profiles, configuration)
- Avoid loading entire job list - paginate when possible

## Common Pitfalls to Avoid
- Don't assume selectors will stay stable - job sites change often
- Don't skip error handling in automation flows
- Don't forget to update both frontend and backend when changing data models
- Don't ignore TypeScript errors - fix them properly
- Don't add dependencies without considering bundle size

## When Making Changes
- Check if similar functionality already exists
- Consider backward compatibility with existing database data
- Update related tests
- Check if documentation needs updating
- Test manually in dashboard if UI is affected

## System-Specific Features

### Job Status Lifecycle
Six-state system: queued → applied → interview/rejected, plus skipped and reported
- **queued**: High-scoring jobs ready for application
- **applied**: Successfully submitted applications
- **interview**: Moved to interview stage (manual update)
- **rejected**: Application rejected (triggers learning system)
- **skipped**: Low score or failed application
- **reported**: Moved from queued for manual review, blocks auto-application

### Rejection Learning System
Located in `src/ai/rejection-analyzer.ts`, `rejection-filters.ts`, `weight-manager.ts`
- Analyzes rejection reasons using keyword patterns and LLM
- Adjusts profile category weights immediately (e.g., "too junior" → increase seniority weight)
- Builds company blocklists for repeated rejections
- Filters jobs before ranking using learned patterns
- Dashboard displays active adjustments and learning history
- When modifying: ensure backward compatibility with existing learning data

### Job Ranking & Scoring
Located in `src/ai/ranker.ts` and `profiles.ts`
- Current weights: Core Azure/API 20%, Security 15%, .NET 20%, Event-Driven 10%, Performance 10%, Frontend 10%, Legacy Modernization 5%, DevOps 0%
- MIN_FIT_SCORE default: 70 (configurable in .env)
- Profiles contain Boolean search queries and keyword lists per category
- Available profiles: core, security, event-driven, performance, devops, backend, core-net, legacy-modernization
- When changing weights: consider impact on existing queued jobs, may need re-scoring

### AI Content Generation
- **Headlines** (`src/dashboard/routes/headline.ts`): One-sentence professional summaries for applications
- **Cover Letters** (`src/dashboard/routes/cover-letter.ts`): Job-specific letters based on fit analysis
- Both use RAG context from resumes in `resumes/` folder
- Generated on-demand via dashboard, not pre-generated during search
- Use structured prompts with clear instructions and examples

### Intelligent Field Mapping
Located in `src/ai/mapper.ts`
- Three-tier system: heuristics (fast) → cache (medium) → AI (accurate)
- Heuristics: Pattern matching for common fields (email, phone, name)
- Cache: Stores successful mappings in database
- AI: Semantic understanding for complex or ambiguous fields
- When modifying: test with all three tiers, ensure cache invalidation works

### Selector Learning System
Located in `tests/learning-system/`
- Captures successful CSS selectors from form fills
- Stores in database for reuse
- Improves reliability over time as more forms are filled
- Run tests with: `npm run test:learning`
- When adding new adapters: ensure they integrate with learning system

### ATS Adapters
Located in `src/adapters/`
- Must extend BaseAdapter interface
- Implement: detectJobBoard(), fillApplication()
- Each adapter handles board-specific quirks
- **Greenhouse**: Uses data-qa attributes, multi-step forms
- **Lever**: Single-page, autosaves, custom fields possible
- **Workday**: Uses iframes, slow loading, complex navigation
- Generic fallback for unknown boards

### Dashboard Features
Frontend: `src/dashboard/client/`, Backend: `src/dashboard/`
- Auto-refreshes every 5 seconds using TanStack Query
- HTTPS on localhost (ports 3000/3001)
- Routes: `/stats`, `/jobs`, `/runs`, `/analytics`, `/headline`, `/cover-letter`
- Uses React contexts for navigation state
- Custom hooks in `hooks/` for data fetching
- When adding features: update both frontend and backend, ensure HTTPS works

### MCP Servers Integration
- **Playwright MCP**: Interactive browser debugging (snapshots, clicks, network monitoring)
- **Context7 MCP**: Live documentation for Playwright, TypeScript, React
- **Azure MCP**: Cloud deployment and production monitoring
- Available for debugging complex automation issues
- Use browser_snapshot instead of screenshots for interactive debugging

### Testing Structure
- `tests/`: Main test suites (login, search, mapper, ranker, integration)
- `tests/learning-system/`: Selector learning tests
- Run all: `npm run test:all`
- Run learning only: `npm run test:learning`
- Tests use in-memory SQLite and mock LLM responses
- When adding tests: follow existing patterns, keep isolated

### Data Storage
- `data/app.db`: SQLite database (jobs, applications, learning data)
- `resumes/`: PDF/DOCX files for RAG context
- `storage/storageState.json`: LinkedIn session (gitignored)
- `artifacts/`: Screenshots and Playwright traces for debugging
- `dist/`: Built dashboard assets
- When modifying schema: handle migration carefully (SQLite limitations)

### Configuration Files
- `.env`: Environment variables (MIN_FIT_SCORE, LLM_MODEL, HEADLESS, ENABLE_TRACING)
- `answers-policy.yml`: Controls application form responses (field length, allowed values)
- `docker-compose.llm.yml`: Ollama local LLM setup
- When changing config: update docs and provide migration path

